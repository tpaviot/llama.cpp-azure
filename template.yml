parameters:
  name: Linux
  vmImage: 'ubuntu-22.04'
  model: 'openlm-research/open_llama_3b_v2'

jobs:
- job: ${{ parameters.name }}
  timeoutInMinutes: 360
  
  pool:
    vmImage: ${{ parameters.vmImage }}
    
  steps: 
  
  - bash: |
      sudo apt-get update && \
      sudo apt-get -y install libopenblas-dev ninja-build
    displayName: 'Install OpenBLAS library'

  - bash : |
      git clone https://github.com/ggerganov/llama.cpp.git
    displayName: 'Clone llama.cpp git repository'

  - bash : |
      cd llama.cpp && \
      mkdir cmake-build && \
      cd cmake-build && \
      cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_BLAS=ON  -DLLAMA_BLAS_VENDOR=OpenBLAS -G Ninja .. && \
      ninja
    displayName: 'Build llama.cpp using ninja'

  - bash: |
      cd llama.cpp && \
      pip install -r requirements.txt
    displayName: 'Install python requirements'

  - bash : |
      mkdir model && \
      wget -q -O ./model/config.json https://huggingface.co/${{ parameters.model }}/raw/main/config.json && \
      wget -q -O ./model/tokenizer.model https://huggingface.co/${{ parameters.model }}/resolve/main/tokenizer.model && \
      wget -q -O ./model/tokenizer_config.json https://huggingface.co/${{ parameters.model }}/raw/main/tokenizer_config.json && \
      wget -q -O ./model/special_tokens_map.json https://huggingface.co/${{ parameters.model }}/raw/main/special_tokens_map.json && \
      wget -q -O ./model/generation_config.json https://huggingface.co/${{ parameters.model }}/raw/main/generation_config.json && \
      wget -q -O ./model/pytorch_model.bin https://huggingface.co/${{ parameters.model }}/resolve/main/pytorch_model.bin && \
      ls ./model
    displayName: 'Download ${{ parameters.model }}'

  - bash : |
      cd llama.cpp && \
      python3 convert.py ../model && \
      ls ../model
    displayName: 'Test convert model to ggml F16 format'

  - bash : |
      rm ./model/pytorch*.bin
      rm ./model/tokenizer.model
    displayName: 'Remove model files, no need for them anymore'

  - bash : |
      ./llama.cpp/cmake-build/bin/quantize ./model/ggml-model-f16.gguf ./model/ggml-model-q4_0.bin q4_0
    displayName: 'Test quantize model to Q4_0'

  - bash : |
      wget -q -O ./model/ggml-model-q4_0.bin https://huggingface.co/TheBloke/open-llama-7B-v2-open-instruct-GGML/resolve/main/open-llama-7b-v2-open-instruct.ggmlv3.q4_0.bin && \
      ./llama.cpp/cmake-build/bin/main -m ./model/ggml-model-q4_0.bin -s 1234 -n 64 --ignore-eos -p "I believe the meaning of science is"
    displayName: 'Test infer OpenLlama-3b-v2 Q4_0 quantized model'

  - bash : |
      rm ./model/*
    displayName: 'Remove ggml-f16 file and q4_0 files to save disk space'

  - bash : |
      wget -q -O ./model/ggml-model-q4_k_m.bin https://huggingface.co/TheBloke/Llama-2-7B-GGML/resolve/main/llama-2-7b.ggmlv3.q4_K_M.bin && \
      ./llama.cpp/cmake-build/bin/main -m ./model/ggml-model-q4_k_m.bin -s 1234 -n 64 --ignore-eos -p "I believe the meaning of science is"
    displayName: 'Test infer OpenLLama-7b-v2 Q4_0 quantized model'

  - bash : |
      ./llama.cpp/cmake-build/bin/main -m ./model/ggml-model-q4_0.bin -n 256 --grammar-file ./llama.cpp/grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'
    displayName: 'Test infer grammar'

  - bash : |
      ./llama.cpp/cmake-build/bin/perplexity -m ./model/ggml-model-q4_k_m.bin -f ./reduced_wikitext-2-raw/wiki.test.raw --export
    displayName: 'Test compute perplexity on small wikitext-2 dataset'

  # - task: PublishPipelineArtifact@0
  #   inputs:
  #     targetPath: '/home/vsts/work/1/s/model/'
  #     artifactName: Quantized Q4_0 model ${{ parameters.hf_model_vendor }}/${{ parameters.hf_model_name }}
  #   displayName: 'Publish the quantized Q4_0 artifact'
