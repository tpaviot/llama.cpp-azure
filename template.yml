parameters:
  name: Linux
  vmImage: 'ubuntu-22.04'

jobs:
- job: ${{ parameters.name }}
  timeoutInMinutes: 360
  
  pool:
    vmImage: ${{ parameters.vmImage }}
    
  steps: 
  - ${{ if contains(parameters.vmImage, 'ubuntu') }}: 
    - bash: |
         sudo apt-get update && \
         sudo apt-get -y install libopenblas-dev ninja-build libopenmpi-dev
      displayName: 'Install BLAS and MPI libraries'
  # on macOs, openblas is alreadey installed
  - ${{ if contains(parameters.vmImage, 'macOs') }}: 
    - bash: |
         brew install ninja open-mpi
      displayName: 'Install BLAS and MPI libraries'

  - bash : |
      git clone https://github.com/ggerganov/llama.cpp.git
    displayName: 'Clone llama.cpp git repository'

  - bash : |
      cd llama.cpp/cmake-build/bin && \
      ./test-sampling && \
      ./test-quantize-fns && \
      ./test-quantize-perf
    displayName: 'Run basic tests'

  - bash: |
      cd llama.cpp && \
      pip install -r requirements.txt
    displayName: 'Install python requirements'

  - bash : |
      mkdir model && \
      wget -q -O ./model/pytorch_model.bin https://huggingface.co/openlm-research/open_llama_3b/resolve/main/pytorch_model.bin && \
      wget -q -O ./model/tokenizer.model https://huggingface.co/openlm-research/open_llama_3b/resolve/main/tokenizer.model && \
      ls ./model
    displayName: 'Download OpenLLama-3b'

  - bash : |
      cd llama.cpp && \
      python3 convert.py ../model && \
      ls ../model
    displayName: 'Test convert model to ggml F16 format'

  - bash : |
      rm ./model/pytorch*.bin
      rm ./model/tokenizer.model
    displayName: 'Remove model files, no need for them anymore'

  - bash : |
      ./llama.cpp/cmake-build/bin/quantize ./model/ggml-model-f16.bin ./model/ggml-model-q4_0.bin q4_0
    displayName: 'Test quantize model to Q4_0'

  - bash : |
      rm ./model/*
    displayName: 'Remove ggml-f16 file and q4_0 files to save disk space'

  - bash : |
      wget -q -O ./model/ggml-model-q4_0.bin https://huggingface.co/TheBloke/open-llama-7B-v2-open-instruct-GGML/resolve/main/open-llama-7b-v2-open-instruct.ggmlv3.q4_0.bin && \
      ./llama.cpp/cmake-build/bin/main -m ./model/ggml-model-q4_0.bin -n 256 --repeat_penalty 1.0 --color -r "User:" -f ./llama.cpp/prompts/chat-with-bob.txt
    displayName: 'Test infer OpenLLama-7b Q4_0 quantized model'

  - bash : |
      ./llama.cpp/cmake-build/bin/perplexity -m ./model/ggml-model-q4_0.bin -f ./reduced_wikitext-2-raw/wiki.test.raw --export
    displayName: 'Test compute perplexity on small wikitext-2 dataset'

  # - task: PublishPipelineArtifact@0
  #   inputs:
  #     targetPath: '/home/vsts/work/1/s/model/'
  #     artifactName: Quantized Q4_0 model ${{ parameters.hf_model_vendor }}/${{ parameters.hf_model_name }}
  #   displayName: 'Publish the quantized Q4_0 artifact'
